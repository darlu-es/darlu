<!DOCTYPE html>
<html lang="es">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel='stylesheet' href='../../style.css' type='text/css' media='all' />
  <link rel="icon" href="../../favicon.ico" />

  <title>d.Arlu - La navaja de Ockham</title>
</head>

<body>
  <header class="site-header" role="banner">
    <h3 class="site-title header-text-color">
      <a href="../../index.html" rel="home">d.Arlu</a>
    </h3>
    <h4 class="site-description">Lo que quiero contar, quiere ser escrito.</h4>
    <a href="../../index.html"><u>Escrituras</u></a>
    |
    <a href="../../cajon-de-sastre/index.html"><u>Cajón de sastre</u></a>
    |
    <a href="../../libros/index.html"><u>Librería</u></a>
    |
    <a href="../../info/index.html"><u>Información</u></a>
  </header>

  <article>

    <header class="entry-header">
      <h1 class="entry-title">La navaja de Ockham</h1>
      <div class="entry-meta">
        <span class="posted-on">Publicado: <time class="entry-date published">Diciembre 2020</time></span>
      </div>
    </header>
    <div class="entry-content">

      <p>Dos de las obras maestras arquitectónicas de Barcelona son completamente
  opuestas. La Sagrada Familia, diseñada por Antoni Gaudí, se encuentra a pocos
  kilómetros del pabellón alemán, construido por Mies van der Rohe. La iglesia de
  Gaudí es extravagante y compleja, mientras que el pabellón de Mies es tranquilo
  y sencillo. Mies, el apóstol de la arquitectura minimalista, usó el lema "menos
  es más" para expresar lo que buscaba. Gaudí nunca dijo "más es más", pero sus
  edificios sugieren que eso es lo que tenía en mente.</p>
<p>Sin embargo, entre
  estos dos extremos hay un vasto espacio de posibilidades. Diferentes artistas
  han tenido distintos objetivos pues su trabajo no es tratar de descubrir el
  grado de complejidad único y correcto que deberían tener todas las obras de
  arte. <strong>En el arte, no existe tal ideal atemporal</strong>.</p>
<p>La
  ciencia es diferente, al menos según muchos científicos. Albert Einstein habló
  por muchos cuando dijo que "apenas se puede negar que el objetivo supremo de
  toda teoría es hacer que los elementos básicos irreducibles sean lo más simples
  sin tener que renunciar a la representación adecuada de un solo dato de
  experiencia". La búsqueda de de teorías simples, entonces, es un requisito de la
  comunidad científica. Cuando las teorías se vuelven demasiado complejas, los
  científicos buscan la Navaja de Ockham (el principio de la parsimonia) para
  recortar. Este principio dice que una teoría que postula menos entidades,
  procesos o causas es mejor que una teoría que postula más, siempre y cuando la
  teoría más simple sea compatible con lo que observamos. Pero, ¿qué significa
  "mejor"?</p>
<h2>Las reglas del razonamiento</h2>
<p>Uno de los estandartes de
  la Navaja de Ockham se puede encontrar en de Isaac Newton, donde establece
  cuatro "reglas del razonamiento". Siendo las dos primeras las siguientes:</p>
<ol>
  <li>No se deben admitir más causas naturales de las suficientes para
    explicar el fenómeno. Como dicen los filósofos: la naturaleza no hace nada en
    vano, y más causas son en vano cuando menor son suficientes. Esto es debido a
    que la naturaleza es simple y no se permite el lujo de causas superfluas.</li>
  <li>Por lo tanto, las causas asignadas a los efectos naturales de la misma deben
    ser, en la medida de lo posible, las mismas. El claro ejemplo de esta regla
    puede ser la causa del fenómeno de respiración tanto en hombres como en
    bestias.</li>
</ol>
<p>Newton no dice mucho para justificar esas reglas, pero en
  un comentario inédito sobre el libro se explaya más en "Reglas para metodizar"
  siendo el siguiente parágrafo una de ellas:</p>
<p>> Se deben eligir aquellas
  construcciones que sin esfuerzo reducen los elementos a su máxima simplicidad.
  La razón para ello es que la verdad siempre se puede encontrar en la
  simplicidad, y no en la multiplicidad y confusión de los elementos. Es la
  perfección de las obras de Dios que todas se realicen con la mayor simplicidad.
  él es el Dios del orden y no de la confusión. Y por lo tanto, aquellos que
  entienden la estructura del mundo deben esforzarse por reducir su conocimiento a
  la máxima simplicidad posible.</p>
<p>Newton cree que priorizar aquellas teorías
  más simples tiene sentido, ya sea para interpretar la Biblia o descubrir las
  leyes de la física.</p>
<h2>La simplicidad como método para evaluar el
  mundo</h2>
<p>En el siglo XX, filósofos, estadísticos y científicos progresaron
  en la comprensión de por qué la simplicidad de una teoría es relevante para
  evaluar cómo es el mundo. Sus justificaciones de la Navaja de Ockham no dependen
  de la teología, ni invocan la grandiosa tesis de que la naturaleza es simple.
  Existen tres "paradigmas de parsimonia", situaciones dentro de los cuales se
  puede justificar el uso de la dicha navaja.</p>
<p>El primero de ellos se
  ejemplifica con el consejo dado a los estudiantes de medicina en el que deben
  <em>evitar perseguir cebras</em>. Si los síntomas de un paciente se pueden
  explicar o bien por una hipótesis X donde tiene una enfermedad común, o bien por
  una hipótesis Y donde tiene una enfermedad rara, se debe priorizar X sobre Y. Se
  entiende que X es más parsimonioso y que, por tanto, tiene mayor probabilidad de
  ser cierto.</p>
<p>Otra de las situaciones donde las teorías más simples tienen
  una mayor probabilidad de ser ciertas es cuando se hace uso de la "navaja del
  silencio". En esta variante de la Navaja de Ockham nos dice que si hay evidencia
  de que X1 es causa de E(enfermedad) y ninguna evidencia de que X2 es causa de E,
  entonces X1 es una mejor explicación de E que X2.</p>
<p>>Durante el siglo XIX,
  John Stuart Mill dijo que no debemos creer nada de lo que no hay evidencia. Como
  si tuviéramos que suponer que un hombre que fue asesinado al caer por un
  precipicio, éste también debe haber tomado veneno.</p>
<p>El problema cambia si
  consideramos dos hipótesis conjuntivas. ¿Cuál es la mejor explicación de que
  X1->E y no X2->E o X1+X2->E? La navaja del silencio no nos proporciona ninguna
  orientación en cuanto a esta pregunta y por ello aparece la "navaja de la
  negación". Esta navaja nos dice que debemos priorizar la primera opción pues
  postular una sola causa común es más parsimonioso que postular una gran cantidad
  de causas independiente y separadas pero sumatorias.</p>
<p>En el primer
  paradigma de parsimonia, en el ejemplo de las enfermedades raras y comunes,
  tenemos que las dos hipótesis (una simple y una compleja) confieren la misma
  probabilidad. En el segundo paradigma de parsimonia, tenemos que las dos
  hipótesis confieren diferentes probabilidades. En muchos casos, la evidencia
  favorece la teoría más simple sobre su competidor más complejo. Por ejemplo,
  suponemos que todas las luces del vecindario se apagan al mismo tiempo. Así
  pues, se consideran las dos hipótesis:</p>
<ul>
  <li>Hipótesis 1 (H1): algo le
    sucedió a la planta de energía que influyó en todas las luces.</li>
  <li>Hipótesis 2 (H2): algo le sucedió a cada uno de las bombillas que influyó en
    todas las luces.</li>
</ul>
<p>Postular una sola causa común es más parsimonioso
  que postular una gran cantidad de causas independientes y separadas. El
  oscurecimiento simultáneo de todas esas luces es más probable si H1 es verdadero
  que si H2 fuera cierto.</p>
<p>> Un ejemplo biológico en el que se prefieren las
  causas comunes a las causas separadas se puede encontrar en la hipótesis de
  Charles Darwin donde toda la vida actual se remonta a uno o algunos progenitores
  originales. Los biólogos modernos están en la misma página cuando señalan que la
  casi universalidad del código genético favorece fuertemente la hipótesis de la
  ascendencia común sobre la hipótesis de antepasados múltiples.</p>
<p>Según el
  tercer paradigma de parsimonia, la parsimonia es relevante para estimar con qué
  precisión un modelo predecirá nuevas observaciones. Hirotugu Akaike presentó un
  teorema sorprendente que demostró esta relevancia. Este teorema, denominado
  "teoría de selección de modelo" es la base de un criterio de evaluación que se
  denominó AIC (Criterio de información de Akaike). AIC dice que la capacidad de
  un modelo para predecir nuevos datos se puede estimar al ver qué tan bien se
  ajustan a los datos antiguos y al ver cuán simple es.</p>
<p>Por ejemplo, te
  encuentras conduciendo por un camino rural con dos enormes campos de maíz, uno a
  cada lado del camino. Detienes el coche y escoges una muestra de 100 plantas de
  maíz de cada campo y obtienes un promedio de 130 centímetros en la muestra del
  campo 1 y un promedio de 140 en la muestra del campo 2. Como te encuentras al
  final del ciclo de crecimiento de la temporada de maíz, asumes que las alturas
  promedio de los dos campos no variaran en los próximos días. Así pues, te
  comprometes venir mañana y volver a escoger una muestra de 100 plantas de maíz
  de cada campo. ¿Cuál de las siguientes dos predicciones crees que será más
  precisa?</p>
<ul>
  <li>Predicción A: las 100 plantas que muestre mañana de la
    primera población tendrán un promedio de 130 centímetros y las 100 plantas que
    muestre mañana de la segunda población tendrán un promedio de 140
    centímetros.</li>
  <li>Predicción B: cada una de las dos muestras tendrán un
    promedio de 135 centímetros.</li>
</ul>
<p>El teorema de Akaike nos dice que
  este problema se puede resolver considerando los siguientes dos modelos sobre
  altura promedio de las dos poblaciones:</p>
<ul>
  <li>DIFF: en este modelo la
    altura promedio en la primera población (h1) es diferente, o no, a la de la
    segunda población (h2).</li>
  <li>NULL: en este modelo la altura promedio en la
    primera población y en la segunda es la misma (h).</li>
</ul>
<p>En ninguno de
  los dos modelos nos dice cuáles son los valores de h1, h2, y h; estos parámetros
  son los denominados "ajustables". El modelo NULL tiene ese nombre porque dice
  que las dos poblaciones no difieren en sus alturas promedios. El modelo DIFF es
  un poco engañoso ya que el modelo no dice que las dos poblaciones deban diferir
  sí o sí en sus alturas promedios, sino que DIFF permite esa posibilidad, pero
  también permite que las dos poblaciones tengan la misma altura.</p>
<p>¿Qué
  predicen DIFF y NULL sobre los datos que extraerá de los dos campos mañana? Los
  modelos por si sólo no proporcionan valores, sin embargo se puede ajustar cada
  modelo a sus datos anteriores estimando los valores de los parámetros ajustables
  (h1, h2, y h). El resultado son los siguientes dos modelos ya ajustados
  (fitted:f):</p>
<ul>
  <li>f(DIFF): h1 = 130 centímetros y h2 = 140
    centímetros.</li>
  <li>f(NULL): h = 135 centímetros.</li>
</ul>
<p>La pregunta de
  qué modelo predecirá con mayor preción los nuevos datos se interpreta como:
  <em>¿Qué modelo, una vez se han ajustado a los datos antiguos, predecirá con
    mayor precisión los nuevos datos que aún no se tienen?</em></p>
<p>Puedes pensar
  que DIFF tiene que ser cierto y que NULL tiene que ser falso pues ¿cuáles son
  las posibilidades de que dos grandes poblaciones de plantas de maíz tengan
  exactamente la misma altura promedio? Si la pregunta fuese que modelo es cierto
  y cuál es falso ya habríamos terminado, pero esa no es la pregunta pues lo que
  deseamos es evaluar los dos modelos por su precisión predictiva.</p>
<p>Uno de
  los hechos sorprendentes sobre los modelos NULL y DIFF es que un modelo que se
  sabe que es falso a veces hará predicciones más precisas que un modelo que se
  sabe que es verdadero. NULL, pese a ser falso, puede que se acerque más a lo
  cierto. Si esto fuera así, sería mejor usar NULL para predecir nuevos datos en
  lugar de utilizar DIFF. Después de todo, los datos anterior con los que
  ajustamos los modelos puede que no sean representativos. De tal forma que NULL
  nos mantiene rectos, mientras que DIFF nos invita a desviarnos.</p>
<p>> Como ya
  hemos ido comentando al principio del artículo, <strong>es una cuestión de gusto
    en cuanto la importancia de la simplicidad y la complejidad en el arte. Pero la
    simplicidad, en ciencia, no es cuestión de gustos</strong>.</p>
<p>El Criterio
  de Información de Akaike que hemos introducido antes, evalúa NULL y DIFF
  teniendo en cuenta dos hechos: f(DIFF) se ajusta mejor a los datos antiguos que
  f(NULL), pese a que DIFF es más complejo que NULL. Aquí la complejidad de un
  modelo es la cantidad de parámetros ajustables que contiene el modelo. Como
  mencioné, AIC se basa en el teorema de Akaike, que puede describirse
  informalmente de la siguiente manera:</p>
<blockquote class="wp-block-quote">
  <p>Una estimación imparcial de la precisión predictiva de una modelo (M) estable
    que: M = (qué tan bien se ajusta f(M) a los datos antiguos) – (el número de
    parámetros ajustables que contiene el modelo M).</p>
</blockquote>
<p>El
  resultado matemático de esta fórmula, por lo tanto, puede establecerse que la
  parsimonia es relevante para estimar la precisión predictiva.</p>
<p>El teorema
  de Akaike, como su propio nombre indica, es un teorema lo que significa que éste
  deriva de suposiciones. Existen tres suposiciones. La primera es que los
  conjuntos de datos antiguos y nuevos se generan a partir de la misma realidad
  subyacente, este supuesto se cumple en nuestro ejemplo si la altura promedio de
  cada población permanece sin cambios a medida que se dibujan los conjuntos de
  datos antiguos y nuevos. La segunda suposición es que las estimaciones repetidas
  de cada uno de los parámetros en un modelo formarán una distribución en forma de
  campana de Gauss. La tercera suposicion es que uno de los modelos que compiten
  es verdadero o que está cerca de la verdad. Esta suposición se cumple en el
  ejemplo del maíz, ya que NULL o DIFF deben ser verdaderas.</p>
<p>Gaudí y Mies
  nos recuerdan que no hay disputas de gustos a la hora de evaluar el valor de la
  simplicidad y la complejidad en las obras de arte. Einstein y Newton nos dicen
  que la ciencia es diferente. Reichenbach y Akaike nos proporcionan algunas
  razones por las cuales esto es así. El resultado es que hay tres paradigmas de
  parsimonia que explican cómo la simplicidad de una teoría puede ser relevante
  para decir cómo es el mundo:</p>
<ul>
  <li>Paradigma 1: a veces las teorías más
    simples tienen mayores probabilidades.</li>
  <li>Paradigma 2: a veces las teorías
    más simples están mejor respaldadas por las observaciones.</li>
  <li>Paradigma 3:
    a veces la simplicidad de un modelo es relevante para estimar su precisión
    predictiva.</li>
</ul>
<p>Estos tres paradigmas tienen algo importante en común.
  Si un problema encaja en alguno de los tres paradigmas, ésto depende de
  suposiciones sobre el problema. Estas suposiciones pueden ser ciertas para
  algunos problemas, pero falsas para otros. Aunque la parsimonia es relevante
  para formar juicios sobre cómo es el mundo, al final no existe una justificación
  incondicional y sin presunciones para la Navaja de Ockham.</p>
<h2>—</h2>
<p>Debo mejorar el artículo en futuras revisiones. Algunos comentarios y
  artículos a tener en cuenta:</p>
<p>The principle is very misleading and has
  been seriously (if typically implicitly) invoked in trying to generate
  hypotheses or explanations in genetics and evolutionary biology. But the core
  theory of life is based on random factors (e.g., mutation) and ad hoc local
  mixes of `causal' factors. The multiplicity of simultaneous factors in local
  ecologies means that simplicity is in a sense not just inapt but a wrong
  hypothesis or assumption. Statistical replicability (normal distribution
  referred to here, or the three principles) cannot be assumed. Occam's razor
  works very well for sciences and situations where replicability is a reasonable
  assumption. But that's not always accurate for understanding life. There,
  perhaps Gaudi was closer to the truth–at least, he's more interesting.</p>
<hr class="wp-block-separator" />
<p>I don't think it's a surprise or a problem that
  there's no unconditional justification. It can't, after all, be the case that
  the simplest explanation is always true. If we take any true explanation it's
  usually quite easy to come up with one that would be simpler – but wrong. The
  law of gravity explains the motion of the moon, but it would be simpler to think
  that heavenly bodies always move in perfect circles at a constant rate. Simpler,
  but false. So it's not a worry that Ockham (Occam?) only offers provisional
  guidance. It's not a method of proof, just a practical maxim about not wasting
  time considering entities that don't help.</p>
<p>After all, for every set of
  observations there is an infinite set of theories that perfectly account for
  them. The moon's motions are explained by gravity, but we could also say that
  the force of gravity is transferred by Gravitas the invisible angel; or by a
  whole chain of undetectable angels as long as we can be bothered with. We can't
  prove that Gravitas does not exist; it's even conceivable that one day if he
  gets slapdash we will begin to see actual evidence of his interventions; but for
  now writing him into sums where he doesn't change anything is just a waste of
  time.</p>
<p>I think the three paradigms are intriguing, and clearly simplicity
  is not itself a simple matter. Is it simpler to think that the universe is far
  vaster than it seems; or smaller but with five dimensions?The former might
  involve more entities (lots of stars) but the latter seems less parsimonious in
  some more fundamental way. Setting aside the truth for a moment, which is
  actually simpler – believing that light is particles or believing that it is
  waves? My mind is boggled: but I do think that in the end it comes down to not
  wasting time</p>
<hr class="wp-block-separator" />
<p>I see the application of
  Ockam's razor in all kinds of situations, mostly by parents when trying to keep
  their children from "going off the deep end" when studying for their school
  homework. In their mad rush to try and understand a complicated subject, these
  children over-complicate things and start taking the shotgun approach to the
  problem, to which the parents admonish their children by saying, "Whoa! Just
  settle down a moment here, young man! It isn't as all complicated as you make it
  out to be. Let's just take this one step at a time". Parents don't call this
  Ockam's Razor, but that is exactly what it is. Many of those children directed
  by their parents to "just settle down", will grow up to be scientists, who even
  if they have never heard of Ockam's Razor, are predisposed to practice it due to
  childhood training.</p>
<p>Is the shotgun approach a bad thing? No, it is how
  much of modern science is performed in today's world. It isn't the most
  intelligent approach, but it can sometimes get results when no other method will
  work (or when you have no choice, such as when no more intelligent enough
  scientists can be found). The military often uses the shotgun approach, i.e. –
  whenever they have a problem, they throw a hundred men at it, and keep throwing
  men at it until, one way or another, it gets resolved.</p>
<p>Okkam's Razor is
  an attempt to keep the shotgunning approach to a minimum, because if things like
  Okkam's Razor didn't exist, many organizations facing difficult problems or
  pressure to produce results, would try to take the easy way out by resorting to
  shotgunning. While shotgunning doesn't hinder scientific progress, it doesn't
  promote it as effectively as intelligent inquiry does. Ockam's Razor cannot be
  justified, but neither can it be dismissed.</p>
<hr class="wp-block-separator" />
<p>See "The Myth of Simplicity", Mario Bunge, 1963.</p>
<p>"In the present case,
  the opponents are friends in the large and complex camp of illuministic
  philosophy, by which I understand the philosophy that at least wishes to make
  friends with logic and science and rejects obscurantism. I shall call those
  friends whom I am criticizing philosophic dadaists and cubists. By philosophic
  Dadaism I mean the seraphic belief in simple ultimates, in the necessity of
  drastically reducing the number of basic concepts, in the existence of entirely
  safe entities (e.g., sense data), and in the possibility of depicting every
  theory about theories with a few simple strokes. Philosophic Cubism, very close
  to Dada, operates in the construction of rigorous but utterly artificial
  "languages" or theories, remote from living science and therefore of little
  interest in the understanding of knowledge. Both philosophic dadaists and
  philosophic cubists are stern believers in the Myth of Simplicity.</p>
<p>Elegant and logically clean as they are, philosophic Dadaism and Cubism are
  neither the right answer to the hollow rhetoric of philosophic Baroque nor the
  right reaction against the uneventfulness of philosophic Classicism. Not a
  trivialization of problems and an impoverishment of ideas, but a deepening and
  renewal of both are required to, counteract Baroque and Classicism. In
  philosophy, even more than in art, an experimental and critical realism is the
  sound and fruitful view. The desirable complexity that accompanies both depth
  and span can then be opposed to the redundancies of Baroque, and the critical
  alertness that secures progress can then be proposed in place of the rigidity of
  Classicism. The oversimplification of Dadaism and the artificiality of Cubism
  can thereby be avoided.</p>
<p>All oversimplification should be avoided in
  science and in philosophy, except as a temporary methodological device enabling
  us to start work or to apply its results. Simplicity, a practical and aesthetic
  desideratum, becomes dangerous if promoted to the rank of universal norm,
  particularly in matters of knowledge. For here it can consecrate ignorance and,
  what is worse, the refusal to learn– i.e., dogmatism."</p>
<hr class="wp-block-separator" />
<p>David MacKay's online book ITILA (<a href="http://www.inference.phy.cam.ac.uk/itila)">http://www.inference.phy.cam.ac.uk/itila)</a>
  chap 28 (<a href="http://www.inference.phy.cam.ac.uk/mackay/itprnn/ps/343.355.pdf)">http://www.inference.phy.cam.ac.uk/mackay/itprnn/ps/343.355.pdf)</a>
  gives the clearest justification for Ockham I have seen, as a simple consequence
  of the laws of probability.</p>
    </div>

  </article>

</body>

</html>
